

```
using index

和 

using index condition 不一样

前者是覆盖索引， 后面是索引下推，虽然都是减少回表。
```



```
context 为什么是线程安全
```



```
WithValue 创建 context 节点的过程实际上就是创建链表节点的过程。两个节点的 key 值是可以相等的，但它们是两个不同的 context 节点。查找的时候，会向上查找到最后一个挂载的 context 节点，也就是离得比较近的一个父节点 context。所以，整体上而言，用 WithValue 构造的其实是一个低效率的链表。

所以现在都在自己的包中定义一个类型别名，用来进行key 的比对
```



```
context interface {
  Deadline() (deadline time.Time, ok bool)
	Done() <-chan struct{}
	Err() error
	Value(key interface{}) interface{}
}

context 包包含的方法

context.WithValue() 返回了一个context interface 本质上 value context，定义了上面4种基本方法

context.WithTimeout()

context.WithDeadLine()

context.WithCancel()



```





* 查询过程

  ```
  server :
  
  连接器 -》 
  查询缓存 （通过第一个关键字 select，但但凡当前表的任意一行记录修改，都会清空，mysql 8 已经废除了这个功能，之前的版本可以主动关掉） -》
  解析器（词法分析，语法分析，语法树， 检查关键字）-》
  预处理 （判断表是否存在）-》
  优化器-》
  
  执行计划-》
  
  执行器-》
  
  存储引擎 : 调用 api 接口  -》（innodb, myisam）
  
  现在mysql 默认存储引擎：innodb， 默认索引结构：B+
  ```

* 索引的实现在 存储引擎层 （所以 innodb 和 myisam 支持的索引类型不同，实现索引方式也不同，比如都是 b+ 索引）。 server 层主要是 mysql 函数， 视图等通用功能。

* 关于 mysql 配置中比较重要的几个

  ```
  wait_timeout , 最大空闲时间， 超过这个时间会 mysql发起 主动断开。
  
  max_connecions, 单库的最大连接数， 这个是基于数据库的，一般配置的是 2k 。
  
  go 的 mysql 包中， max_life_time, 单个链接的最大声明周期， 主动kill。防止长链接导致的内存不释放
  ```

  

* 什么是索引下推

  ```
  用到了覆盖索引 。
  
  索引中包含的数据，可以直接在存储引擎层用于 where 条件的判断，减少回表的次数。
  （本来应该是 回表后整条数据从存储引擎给server ，server 中 判断where 来决定这条记录是否合法， 减少了无效的回表次数）
  ```

* 执行器和存储引擎的交互过程

  ```
  select * from product where id = 1; (type = const) (找到第一条记录，就直接结束)
  
  1.执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为 InnoDB 引擎索引查询的接口，把条件 id = 1 交给存储引擎，让存储引擎定位符合条件的第一条记录
  
  2.存储引擎通过主键索引的 B+ 树结构定位到 id = 1的第一条记录，如果记录是不存在的，就会向执行器上报记录找不到的错误，然后查询结束。如果记录是存在的，就会将记录返回给执行器；
  
  3.执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则发送给客户端，如果不符合则跳过该记录
  
  4.执行器查询的过程是一个 while 循环，所以还会再查一次，但是这次因为不是第一次查询了，所以会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为一个永远返回 - 1 的函数，所以当调用该函数的时候，执行器就退出循环，也就是结束查询了。
  
  
  select * from product where name = 'iphone'; (type = all) （找到第一条记录，继续往下寻找，直到遍历完毕）
  
  1.执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 all，这个函数指针被指向为 InnoDB 引擎全扫描的接口，让存储引擎读取表中的第一条记录；
  
  2.执行器会判断读到的这条记录的 name 是不是 iphone，如果不是则跳过；如果是则将记录发给客户的（是的没错，Server 层每从存储引擎读到一条记录就会发送给客户端，之所以客户端显示的时候是直接显示所有记录的，是因为客户端是等查询语句查询完成后，才会显示出所有的记录）
  
  3.执行器查询的过程是一个 while 循环，所以还会再查一次，会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 all，read_record 函数指针指向的还是 InnoDB 引擎全扫描的接口，所以接着向存储引擎层要求继续读刚才那条记录的下一条记录，存储引擎把下一条记录取出后就将其返回给执行器（Server层），执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端；
  
  4.一直重复上述过程，直到存储引擎把表中的所有记录读完，然后向执行器（Server层） 返回了读取完毕的信息；
  
  5.执行器收到存储引擎报告的查询完毕的信息，退出循环，停止查询
  ```

  

* mysql 的 基本存储是怎么样的

  ```
  最小处理单位是 行，
  
  读取的最小单位是 页，大小：16kb, 
  
  区: 是页的合计，为了防止随机io， 把页物理存储也合在一起，大小是 1mb.
  
  断：
  
  表空间。
  
  innodb 行存储格式，有多种，举例 compact
  
  
  变长字段列表，null 字段列表， 记录头信息， row_id, trx_id, roll_prt,  列1， 列2， 列3.
  
  变长字段对应 varchar 类型，存储该字段实际长度。 字段长度 > 255 存储， 用2 字节存储 。每一列单独存储。（倒序）
  
  null 列表代表那些可以为 null 的字段，每位存储是否是null， 超过8位，2字节表示。（倒序）
  
  记录头信息， 是否delete， next_record (页内单链表)， record_type (0 普通记录， 1索引， 2 最小记录，  3 最大记录)、
  
  row_id, 没有主键的时候唯一标识。
  
  trx_id, mvcc 时候事务id
  
  roll_pointer, mvcc 向前事务查询。
  ```

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/img/20230125193022.png)



* mysql 的 null 怎么存储的

  ```
  有单独的列存储， 1 代表null， 0 代表 非null。 该列只存储 可能为 null 的情况
  ```

  

* 不定长字段最大存储 66635 长度，到底真实能存储多少字符

  ```
  以 ascii 为例。varchar 里面代表的是字符长度。
  
  非定长列 1个，> 255 占用 2字节。
  
  可能为 null， 占用 1字节
  
  65535 - 2 -1  = 65532 字节。
  
  66535 /1 = 66535 (ascii 字符)
  
  66535 / 3 = 22178 (utf8 字符)
  
  66535 / 4 = 16633 (utf8mb4 字符)
  
  
  页最大16 kb, 小于 656635 b， 超出部分就是页 溢出，类似链表。
  
  ```

* 存储引擎是什么

  ```
  所谓的存储引擎，说白了就是如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法。
  ```

* 索引是什么

  ```
  索引的定义就是帮助存储引擎快速获取数据的一种数据结构，形象的说就是索引是数据的目录。
  ```

* 索引的分类

  ```
  物理结构： 聚簇索引 （叶子节点包含整条数据内容），二级索引 （只存储索引内容 + 主键内容）
  
  数据结构： b+ 索引，  hash 索引， 全文索引
  
  字段特性： 主键索引 （每张表只能有一个，不为 null， 且不同）， 唯一索引 （可以为null）， 普通索引， 前缀索引
  
  字段个数： 单列索引， 联合索引
  ```

* 什么时候适合索引

  ```
  1. 数据量大
  
  2. where 条件，group by  order by 中用到。
  
  3. 区分度够大
  
  4. 唯一性限制。
  ```

* 索引优化

  ```
  1.覆盖索引 （索引下推， use index extra）
  
  2.主键索引最好是自增的 （顺序插入，防止页分裂）
  
  3. 最左前缀， 防止索引失效
  
  4. 前缀索引，减少索引量
  ```



![B+树结构](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/img/20230125221324.png)

叶子节点之间 双向链表， 叶子节点中单链表连接。

非叶子节点中每条记录存储 索引值 和对应的页码。

最大，最小记录 是 row 的 元信息中存储，代表是 最大，最小记录，还是普通记录，还是索引。



![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/img/20230125224621.png)

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/img/20230125225646.png)



上面图更能反应 b+树

1.页目录是数组形式，方便二分查找。

2.页目录中存储每组数据的最大记录。

3.每组数据是单链表连接，有数量限制。



* b+ 索引相比 b, 二叉树， hash 索引有哪些优点

  ```
  相比较 b树，B+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。
  另外，B+Tree 叶子节点采用的是双链表连接，适合 MySQL 中常见的基于范围的顺序查找，而 B 树无法做到这一点。
  
  
  对于有 N 个叶子节点的 B+Tree，其搜索复杂度为O(logdN)，其中 d 表示节点允许的最大子节点个数为 d 个。
  在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 3~4 层左右，也就是说一次数据查询操作只需要做 3~4 次的磁盘 I/O 操作就能查询到目标数据。
  而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 O(logN)，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。
  
  
  Hash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。
  但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。
  ```

* 关于 key 长度的计算

  ```
  explain 执行计划是在server 层，所以比较粗糙。
  
  int 站4 字节， 32 位。 索引长度是 4.
  
  varchar(30) 30字符， utf8mb4, 120 字节， 可能为 null，占用1 字节， 变长字段存储， 2字节 （server 不判断是否 > 255）。
  
  ```

* 索引失效

  ```
  a, b 存在联合索引
  
  Q1: select * from t_table where a > 1 and b = 2，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？
  a 用到了索引， b 无序，没用到。
  
  Q2: select * from t_table where a >= 1 and b = 2，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？
  a 用到了索引， a = 1 的是不，b 用到了索引， 所以 a b 字段都用到了索引
  
  
  Q3: SELECT * FROM t_table WHERE a BETWEEN 2 AND 8 AND b = 2，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？
  between and  ab都用到了索引
  
  Q4: SELECT * FROM t_user WHERE name like 'j%' and age = 22，联合索引（name, age）哪一个字段用到了联合索引的 B+Tree？
  ab 都用到了索引
  
  ```

* explain  type 的类型

  ```
  All（全表扫描）；
  index（全索引扫描）；  %zz 可能用到，如果 select id from table where name like '%zz'
  range（索引范围扫描）； 索引效率提升
  ref（非唯一索引扫描）； 可能不止有一条
  eq_ref（唯一索引扫描）；联表情况
  const（结果只有一条的主键或唯一索引扫描）。
  ```

* 索引有哪些优缺点

  ```
  索引的优点
  
  可以大大加快数据的检索速度，这也是创建索引的最主要的原因。
  通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。
  
  
  索引的缺点
  
  时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率；
  
  空间方面：索引需要占物理空间。
  ```



* mysql 单表存储上限。

  ```
  单页 16kb 大小
  
  1kb 存储 页目录等信息， 15kb 存储数据。
  
  索引为例，15kb / 15b (bigint = 8 主键， 其他字段算 7) ~ 1000.
  
  两层索引 100_0000,
  
  叶子结点，算1k,  15kb/ 1k = 15.
  
  100_0000 8 15 = 1500_00000。 差不多就是这样算的。
  
  树高3层。
  
  ```

  

* 索引失效的情况

  ```
  1. like '%xx' 或者 like '%xx%'
  
  2. 索引列中使用 函数
  
  3. 索引列中使用表达式
  
  4. 联合索引 最左匹配原则， 如果 遇到 > 就会失效
  
  5. 字符串和 数字比较， 数字的优先级比较高，会优先把 字符串转换成数字， 如果作用在表字段上，会使用cast 函数，导致索引失效
  
  6. where 查询句中， or 前的条件是索引列， or 后不是 索引列， 导致索引失效， 需要全表扫。 如果都是 索引列，可以用到 merge。
  ```

* 关于 like 'xx%' 依旧走联合索引的情况

  ```
  虽然丧失了最左原则， 但是如果不用回表查询，还是会通过二级索引去查询，相比 聚簇索引， 二级索引少存储很多东西，比如 trx_id, roll_pointer 这些数据，成本更小。
  
  但前提是不用回表。
  ```

  

* count(*), count(1) , count(id) 和 count(字段) 对比。

  ```
  count(*) --》 count(0) 和 count(1) 一样。如果不存在 二级索引，就遍历聚簇索引，直接 +1
  
  count(id) 也是如果存在二级索引，优先二级索引，其次聚簇索引， 但需要判断 id 不是 null， 就 +1
  
  count(字段)， 如果不加索引，就全局扫，最好加索引，用来遍历二级索引。
  
  
  myisam 不支持事务，不用考虑 mvcc，是表锁。所以可以用一个变量存储 表中保存 count 数据
  
  innodb 需要遍历。
  
  如果count() 带上where 条件， 那 myisam 和 innodb 都要遍历
  
  
  如果不需要很明确的count 值，可以利用
  
  1.redis 计数
  2.innodb 新增表记录
  3.show table status （误差比较大），或者 explain执行计划看扫描多少行
  
  上面这些方案其实都不能加where 条件查询，没多大意义。
  ```

  

* 索引的底层实现

  ```
  Hash索引
  
  基于哈希表实现，只有精确匹配索引所有列的查询才有效，对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），并且Hash索引将所有的哈希码存储在索引中，同时在索引表中保存指向每个数据行的指针。
  
  
  B+Tree索引 (感觉最明显的对比就是 b+ 的非叶子不存储额外的非索引列的信息)
  
  是BTree的改进版本，同时也是数据库索引索引所采用的存储结构。
  1.（范围查询更优）数据都在叶子节点上，并且增加了顺序访问指针，每个叶子节点都指向相邻的叶子节点的地址。相比BTree来说，进行范围查找时只需要查找两个节点，进行遍历即可。而BTree需要获取所有节点，相比之下B+Tree效率更高。
  2. 存储量更多。 非叶子节点只存储索引值，想比 b树的存储整个数据，每页包含数据更多，树的高度更低， io次数更少。
  3. 含有大量重复冗余节点， 增加删除 树结构变化比较小。
  
  ```

* 为什么索引结构默认使用b+ ,而不是b-, hash, 二叉树，红黑树

  ```
  相比hash， 可以范围查询
  
  相比 二叉树， 树的高度更低 （每一个树高都是一次 磁盘io 交换）， 二叉树也不能自平衡
  
  相比 b 树。
  1. 叶子节点包含了所有数据，且叶子结点之间双向链表，适合范围查询。
  2. 非叶子节点只包含索引值，页存储内容更多，树高更低，
  3. 非叶子节点包含的索引值在 叶子节点中都包含，有大量冗余数据，在删除修改节点的时候树结构变动比较小
  
  相比红黑树
  1.自平衡算法更简单。
  ```

  

* 事务的特性

  ```
  只有 innodb 才有事务
  
  myisam 不支持事务。
  
  
  原子性。 整个事务执行要么成功，要么失败。 （通过 undo log 保证 回滚能力）
  
  一致性。 比如转账，转账前 后， 两者总和一致。 （undo + redo + mvcc 保证）
  
  隔离性。 隔离性可以防止多个事务 并发  执行时由于交叉执行而导致数据的不一致  （mvcc 保证）
  
  持久性。事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。（通过 redo log 保证）
  
  
  ```

* 并发事务会导致的问题

  ```
  1.脏读
  2.不可重复读
  3.幻读
  ```

* 为了防止上面事务并发导致的问题， sql 出台了 4种隔离级别来防止问题的发生

  ```
  1.读未提交 （下面三种问题都没办法解决）
  2.读已提交  （解决了脏读）（每次读取前 生成一个快照）
  3.可重复读  （解决了脏读， （第一次读取生成一个快照）不可重复读 通过mvcc， 大概率避免幻读 通过 next-key lock (记录锁 + 间隙锁)）
  4.串行化。
  ```

  

*  mvcc  是什么 (下面都是 解决 select  快照读)

  ```
  多版本控制。
  
  对于不可重复读问题 （两次读取数据内容不一样）
  
  每次事务开始前建立一个快照。
  
  解决方法：
  
  包含
  
  当前事务id
  
  活跃的事务id
  
  最小的活跃事务id <
  
  下一个可分配最大事务id >=
  
  
  比如一条记录，
  
  record 1.
  
  trx_id = 50
  
  
  事务A 快照。
  
  当前事务id :51
  
  活跃的事务id : 51
  
  最小的事务 id: 51,
  
  下一个可分配：52.
  
  
  事务B 快照。
  
  当前事务id :52
  
  活跃的事务id : 51， 52
  
  最小的事务 id: 51,
  
  下一个可分配：53.
  
  
  可重复读：
  
  A 读取record 1,  51 > 50 ,所以能读取到。
  
  B 修改 record 1, record 1 的 trx_id = 52.
  
  A 再次读取 record 1,  52 >= max 下一个可分配，没有提交， 读取 不到，通过 undo log 往前找。读取到旧值。
  
  B 提交
  
  A 依旧读取不到 （保持第一次的 read view）
  
  
  读已提交：
  
  A 读取record 1,  51 > 50 ,所以能读取到。
  
  B 修改 record 1, record 1 的 trx_id = 52.
  
  A 再次读取 record 1,  52 在活跃列表，没有提交， 读取 不到，通过 undo log 往前找。读取到旧值
  
  B 提交。
  
  A 读取，重新生成 read view
  
  
  
  事务A 快照。
  
  当前事务id :51
  
  活跃的事务id : 51
  
  最小的事务 id: 51,
  
  下一个可分配：53.
  
  感觉 这种始终读取不到，因为 b 启动就在后面，总是会大于max 分配的事务id.
  
  
  
  
  
  比如一条记录，
  
  record 1.
  
  trx_id = 50
  
  
  事务A 快照。
  
  当前事务id :51
  
  活跃的事务id : 51
  
  最小的事务 id: 51,
  
  下一个可分配：52.
  
  
  事务B 快照。
  
  当前事务id :52
  
  活跃的事务id : 51， 52
  
  最小的事务 id: 51,
  
  下一个可分配：53.
  
  
  事务 a 修改
  
  record 1  事务id: 51
  
  
  事务 b 读取， 读取不到新的值。
  
  事务 a 提交
  
  如果是 读已提交，可以读取到。（基于当前读取重新生成快照）
  
  如果是 可重复读， 不能读取到 （基于第一次读取）
  
  ```

* 幻读可能发生。快照读： select * from table_a; 不会加锁，只会利用 mvcc 去做读取限制。 当前读， 或者 update， delete ： select * from table_a for update 会加间隙锁

  ```
  mvcc + next-key  只能减少 幻读的发生，但不能杜绝
  
  
  情况一：
  
  A                                              B
  
  开始事务
  select * from table_a where id = 5;            
                                             开始事务
                                             insert into table_a(test) // 5,不会被堵塞因为没加锁但不能读取到因为mvcc
                                             提交事务
  
  select * from table_a where id = 5 for update; // 能被读取到，因为当前读
  
  
  
  情况二：
  A                                              B
  
  开始事务
  select * from table_a where id = 5;            
                                             开始事务
                                             insert into table_a(test) // 5,不会被堵塞因为没加锁但不能读取到因为mvcc
                                             提交事务
  
  update table_a set test = 11 where id = 5 ; 
  
  select * from table_a where id = 5; // 能被读取到，因为事务id 被覆盖。
  
  
  幻读发生的原因是开始没有加间隙锁。
  ```



* mysql的锁

  ```
  全局锁。
  整个数据库只能处于可读状态。不能对表进行增删改查， 不能修改表结构，删除表。
  全局锁主要用于数据库的备份，主要是防止不同表执行时间不同，导致最后统计的数据有问题，比如用户余额表先统计， 用户已购商品表后统计，对不上账单。
  其实对于支持 事务，可重复读 的innodb， 可以不用全局表， 可重复读的隔离级别可以解决这个问题。
  
  
  表级锁。
  
  表锁。 比如select * from table_a; 没有走索引
  
  元数据锁。 对表
  
  
  
  
  
  
  ```

  







* mysql 和 redis 数据一致性保证

  ```
  http://www.xiaerblog.com/articles/2021/07/19/1626624020709.html
  
  既然用了缓存，就不能强一致性，毕竟 sql 和 cache 不是原子性操作，如果强制原子性，性能损耗也太大。
  
  弱一致性 --》 最终一致性。
  
  集中式redis 缓存的三个经典模式
  
  1.cache-aside pattern
  
  写数据：写db， 删缓存。 （如果先删除缓存，容易在写db 的时候，又写进去脏的缓存， 读的流量比较大）
  
  读数据： 先读缓存，有直接返回，没有，读db， 设置缓存。
  
  
  2. read/write-through
  
  我们这边的永久缓存用的就是这种方式。对外暴露一个 cache provider， 其实每次写入都会往db 写一条数据
  
  3. write behind 异步写入
  
  异步写入，还能合并 cache 方法。
  
  
  是否可以先操作缓存。 
  
  可以的，延迟双删除。
  删除缓存就可能存在 失败，失败就要重试，可以通过消息队列。
  删除缓存就容易缓存穿透，可以利用single flight 解决这个问题。
  删除缓存，可以通过监听binlog 去解决。这样既不用 担心失败重试的问题，也不用担心每个地方都要侵入代码，多加删除redis 的代码。
  ```









mysql 的事务如何优化提升速度

```
通过减小锁粒度和减少锁的持有时间进行调优
    （1）结合业务场景，使用低级别事务隔离
    （2）sql优化避免行锁升级表锁
    （3）更新等行锁操作放到事务后面，尽量减少持有锁的时间。
        比如我们先创建订单，执行逻辑，最后扣除库存。
```



* explain 分析的时候要注意哪些

  ```
  select
  _type 其实可以根据 sql 语句判断出来大致是不是简单查询
  
  type ： 
  
  all: 全表扫描
  
  range : 范围查询
  
  ref： where条件匹配， 
  
  const 唯一主键匹配， 
  
  
  
  key 
  
  possible_key , 用到的那些索引
  
  rows， 实际查找的行数
  
  key_len, 关联索引的时候知道用到哪些列
  
  extra  一些排序规则 (
  
  防止出现 fileSort, 如果用到 index condition 挺好的， 索引下推， 防止重复回表。
  
  索引下推 感觉类似 覆盖索引的意思，利用索引中 数据去校验 where 条件是否符合
  
  )
  
  ```

* Sql 优化

  ```
  1. 函数操作
  2. 隐式操作 （这个一定要注意，不止是在dms 上会不注意，代码中也容易不注意，特别是php 这种弱类型语言）
  3. 模糊查询 （注意只能 aa%）
  4. 计算操作 （field 我们一定不要随便操作）
  5. 范围查询 （单次查询量过大，会不走索引。可以分成两次查询，这个还蛮实用）
  ```

  

* b 树， b+ 树 是什么

  ```
  二分查找和适合索引，但是二叉树高度如果节点分布不均匀，容易形成链表，
  于是出现了二叉平衡树，因为节点过高，所以出现了 多叉树，b树，也就是二分搜索树。
  b树上节点不重复，每个节点相对独立，范围查询性能较差，非叶子节点包含索引所有内容，相对存储数据量更小，冗余节点少，删除修改树的结构变动比较大， 于是出现了b+树 。只在叶子节点上存储节点value， 然后非叶子节点的值可能与 叶子节点key 值重复
  
  ```

* 添加索引的场景

  ```
  数据检索时在条件字段添加索引
  
  聚合函数对聚合字段添加索引。 group by
  
  对排序字段添加索引
  
  为了防止回表添加索引
  
  关联查询在关联字段添加索引
  ```

* 普通索引和唯一索引有哪些区别

  ```
  唯一值索引不能重复
  
  ```

* 联合索引

  ```
  最左前缀，一定要注意前面如果是有顺序的才能用到后面的索引
  ```

* 各类型占索引大小

  ```
  这里补充一下 key_len 相关知识点： (null + 1 字节， 变长 + 2 字节)
  
  注意 char （指的是 字符，如果换算成字节 * 3）
  
  explain 中的 key_len 列用于表示这次查询中，所选择的索引长度有多少字节，常用于判断联合索引有多少列被选择了。下表总结了常用字段类型的 key_len：
  
  列类型	KEY_LEN	备注
  int	key_len = 4+1	int 为 4 bytes，允许为 NULL，加 1 byte
  int not null	key_len = 4	不允许为 NULL
  bigint	key_len=8+1	bigint 为 8 bytes，允许为 NULL 加 1 byte
  bigint not null	key_len=8	bigint 为 8 bytes
  char(30) utf8	key_len=30*3+1	char(n)为：n * 3 ，允许为 NULL 加 1 byte
  char(30) not null utf8	key_len=30*3	不允许为 NULL
  varchar(30) not null utf8	key_len=30*3+2	utf8 每个字符为 3 bytes，变长数据类型,加 2 bytes
  varchar(30) utf8	key_len=30*3+2+1	utf8 每个字符为 3 bytes，允许为 NULL,加 1 byte,变长数据类型，加 2 bytes
  datetime	key_len=8+1 (MySQL 5.6.4之前的版本)；key_len=5+1(MySQL 5.6.4及之后的版本)	允许为 NULL，加 1 byte
  ```

*  Cardinality

  ```
  这个区分度，一般都不会看，我们都只到 sex 不容易区分，但有一点我们需要了解的是，b+树上的节点都是 页，而不是真实的记录，他这块取的是 页上的所有记录进行统计。们知道了它的值只是一个估值，因此当我们遇到它的值与实际值相差很大时，可以考虑使用：analyze table xxx; 重新获取统计信息。
  ```

* force index， 强制走索引

*  mysql 三大日志

  ```
  https://juejin.cn/post/6860252224930070536
  
  binglog
  redolog
  undolog
  ```



* myisam 和 innodb 的区别

  ```
  1. myisam 不支持事务和行级锁 (表锁)，所以一般用于有大量查询少量插入的场景来使用，而且myisam不支持外键，并且索引和数据是分开存储的。
  
  2. innodb是基于聚簇索引建立的，和myisam相反它支持事务、外键，并且通过MVCC来支持高并发，索引和数据存储在一起。
  ```

* 说下mysql 的索引有哪些，聚簇索引和非聚簇索引又是什么

  ```
  B+树是左小右大的顺序存储结构，节点只包含id索引列，而叶子节点包含索引列和数据，这种数据和索引在一起存储的索引方式叫做聚簇索引，
  
  一张表只能有一个聚簇索引。
  
  假设没有定义主键，
  
  InnoDB会选择一个唯一的非空索引代替，
  
  如果没有的话则会隐式定义一个主键作为聚簇索引
  
  主键之所以用自增是防止插入导致页分裂。（从左到右逐渐递增）
  ```
  
  ![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/img/20210907000551.png)



* 什么是覆盖索引和回表

  ```
  覆盖索引指的是在一次查询中，如果一个索引包含或者说覆盖所有需要查询的字段的值，我们就称之为覆盖索引，而不再需要回表查询。
  
  而要确定一个查询是否是覆盖索引，我们只需要explain sql语句看Extra的结果是否是“Using index”即可。
  
  （这个明明就是 索引下推 hhh）
  ```

* 锁的类型有哪些

  ```
  mysql锁分为共享锁和排他锁，也叫做读锁和写锁。
  
  读锁是共享的，可以通过lock in share mode实现，这时候只能读不能写。
  
  写锁是排他的，它会阻塞其他的写锁和读锁。
  
  平时好像 写锁用的比较多
  
  从颗粒度来区分，可以分为表锁和行锁两种。
  
  表锁会锁定整张表并且阻塞其他用户对该表的所有读写操作，比如alter修改表结构的时候会锁表。（但我们现在业务的平时 也是可以 ddl 语句的， 问问dba 为啥）
  
  行锁又可以分为乐观锁和悲观锁，悲观锁可以通过for update实现，乐观锁则通过版本号实现 （es 中用的比较多）。
  
  所以mysql 中用到的大部分是 悲观锁，颗粒度是 行锁，然后如果没有索引可能就全表锁了
  
  （为了支持 可重复读，又演变出了间隙锁， record 锁 = 间隙锁 + 行锁）
  ```

* 你能说下事务的基本特性和隔离级别么

  ```
  原子性  （一个事务中的操作要么全部成功，要么全部失败， 靠的就是 undo 日志的回滚功能更）
  
  一致性   （据库总是从一个一致性的状态转换到另外一个一致性的状态。比如A转账给B100块钱，假设中间sql执行过程中系统崩溃A也不会损失100块，因为事务没有提交，修改也就不会保存到数据库。​）(代码层面维护)
  
  隔离性   （一个事务的修改在最终提交前，对其他事务是不可见的。​mvcc 利用 快照 和 事务 id 去维护）
  
  持久性  （指的是一旦事务提交，所做的修改就会永久保存到数据库中 ，持久性内存 + redo 日志落库， 事务提交的时候通过redo log --》用户态内存 -》fsync 同步 ）
  
  隔离级别 ：
  
  读未提交， （脏读）
  
  读已提交，  （不可重复读， 靠当前读，每次读取之前做一张快照，就能解决）
  
  可重复读，  （幻读），幻读和 不可重复读主要区别是 不可重复读是数据修改，幻读是数据新增） （快照读，事务开始的时候 就搞一张快照）
  
  串行化
  
  
  innodb 通过 mvcc， 解决了 可重复读下面 幻读问题。
  
  读已提交 （事务中，读取数据，加 读锁， 另一个事务中， 修改同一个数据，写锁，不会阻塞么。
  
  不会，读写语句的时候好像并不会加排它锁和共享锁
  
  ）
  
  ```



* 快照读和当前读的区别

  ```
  ```

* 什么是幻读，什么是 mvcc

  ```
  ```

* 什么是间隙锁

  ```
  ```

* 你们数据量级多大 ？ 分库分表怎么做的

  ```
  分库：
  业务分库。渠道业务和订单业务分库，分布在不同的数据库实例上，不会相互被影响，增加数据库链接的时候也方便扩容。
  读写分离，读从库，写实库。读写延迟。（如果要求高的，读主库。又或者写入延迟队列再执行。调整参数，减少延迟时间）
  
  
  垂直分表
  把一些冷字段，大字段拆出来
  
  水平分表
  1. （某个field hash）首先根据业务场景来决定使用什么字段作为分表字段(sharding_key)，比如我们现在日订单1000万，我们大部分的场景来源于C端，我们可以用user_id作为sharding_key，数据查询支持到最近3个月的订单，超过3个月的做归档处理，那么3个月的数据量就是9亿，可以分1024张表，那么每张表的数据大概就在100万左右。
  
  比如用户id为100，那我们都经过hash(100)，然后对1024（）取模，就可以落到对应的表上了。
  
  2. 时间片 ，容易不均匀
  3. id 步长 （感觉和上面时间片差不多，但相比于时间片应该更均匀）
  ```

* 分表后的id 怎么保证唯一性

  ```
  因为我们主键默认都是自增的，那么分表之后的主键在不同表就肯定会有冲突了。有几个办法考虑：
  
  1.设定步长，比如1-1024张表我们设定1024的基础步长，这样主键落到不同的表就不会冲突了
  2.分布式ID，自己实现一套分布式ID生成算法或者使用开源的比如雪花算法这种 
  3.redis incr
  
  ```
  
* 分表后非shared_key 怎么处理

  ```
  可以做一个mapping表，比如这时候商家要查询订单列表怎么办呢？不带user_id查询的话你总不能扫全表吧？所以我们可以做一个映射关系表，保存商家和用户的关系，查询的时候先通过商家查询到用户列表，再通过user_id去查询。
  打宽表，一般而言，商户端对数据实时性要求并不是很高，比如查询订单列表，可以把订单表同步到离线（实时）数仓，或者基于其他如es提供查询服务。
  ```

* mysql 主从同步怎么做的

   ```
   首先先了解mysql主从同步的原理
   
   master提交完事务后，写入binlog
   slave连接到master，获取binlog
   master创建dump线程，推送binglog到slave
   slave启动一个IO线程读取同步过来的master的binlog，记录到relay log中继日志中
   slave再开启一个sql线程读取relay log事件并在slave执行，完成同步
   slave记录自己的binglog
   
   
   下面这两种就和 kafka 是一样的
   由于mysql默认的复制方式是异步的，主库把日志发送给从库后不关心从库是否已经处理，这样会产生一个问题就是假设主库挂了，从库处理失败了，这时候从库升为主库后，日志就丢失了。由此产生两个概念。
   
   全同步复制
   
   主库写入binlog后强制同步日志到从库，所有的从库都执行完成后才返回给客户端，但是很显然这个方式的话性能会受到严重影响。
   
   半同步复制
   
   和全同步不同的是，半同步复制的逻辑是这样，从库写入日志成功后返回ACK确认给主库，主库收到至少一个从库的确认就认为写操作完成。
   
   ```

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/img/20210907005622.png)



* 主从延迟怎么解决

  ```
  1.强制走主库查询
  2.后台job任务，先写到延迟队列，在处理
  3.调整参数，比如buffer 大小，优化主从延迟的时间
  ```

* mysql redo log 的写入过程

  ```
  ```

* Mysql 主从不一致怎么优化

  ```
  https://blog.csdn.net/z50L2O08e2u4afToR9A/article/details/83067200
  1、利用多线程重放 relaylog （考虑顺序，多线程执行不同表上的更新操作）（relaylog 包括 update， insert ，delete）
  
  ```

* mysql 主从不一致怎么解决

  ```
  强制读主库
  ```

* mysql 的undo 日志原理，中继日志干嘛的

  ```
  ```

* mysql 的 mvcc 实现原理

  ```
  ```

* 死锁的避免

  ```
  1.按照同一个顺序访问对象
  2.事务尽可能简短，访问完释放
  ```

  





id   粉丝_id  明星_id  根据 粉丝id 拆表， 可以查询到 我关注了那些 明星



某个明星有哪些粉丝，反查询肯定不行，











xxxxxxxxxx 通过减小锁粒度和减少锁的持有时间进行调优    （1）结合业务场景，使用低级别事务隔离    （2）sql优化避免行锁升级表锁    （3）更新等行锁操作放到事务后面，尽量减少持有锁的时间。        比如我们先创建订单，执行逻辑，最后扣除库存。

