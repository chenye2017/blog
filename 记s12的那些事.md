S12 压测暴露了不少问题。

### mysql连接数

### 背景：

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/20221104160858.png)

### 过程：

题外话（关于akso平台上对我们有用的信息）：

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/image-20221104161249694.png)



![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/image-20221104161333349.png)

可以看到集群的拓扑结构，比如多少主，多少从，最大链接数。有多少可执行用户，哪些用户有读权限，哪些有写权限，哪些有proxy 权限。

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/image-20221104161535896.png)



上面的告警就是触发了使用率规则。



1.查看监控

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/image-20221104164632993.png)

我们7点开始压测，activeThreads明显增加，很显然是压测导致，但我们8点压测结束，activeThreads 没有降下来，怀疑idle 连接数没有回收。因为一直告警，虽然业务无影响，出于担忧还是重启了服务，重启完链接数正常。

2. 查看那段时间pod 的数量，是否有扩容（因为之前出过因为服务扩容导致redis 流出过大的问题），pod 无扩容，sad。

3. 查看最近三十天连接数变化，考虑是不是最近新的业务代导致。

   ![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/image-20221104165229665.png)

确实有，但没有规律。过去三十天出现过2次，分别是两个从库出现，因为没有到达告警阈值，所以无感知，一段时间后自然恢复。（确定没有发版，因为都是节假日）



4.八股文中经常有说到，query 的内容 没有close 导致链接数没有释放， or 事务begin 后没有 commit 导致链接数没有释放，翻阅代码，确认没有问题。 （现在我们的go lint 中会检测 rows 有没有使用完毕后close， 架构组开发的这个功能真的厉害）

5.陷入沉思，到底是什么原因导致。按照最初的思路， 压测期间流量太大，active pool 的数量一直增多，但是没有释放，所以什么时候释放？记得有个配置 maxIdleTimeout ,查询后台这个配置，4h，观察前30天自动恢复时间，都是 4h，很精准，问题明确了，就是突发流量导致的conn 变成了idle conn 没有被回收。

6.怎么解决 ？架构组建议迁移 db-proxy。



db-proxy 的优点：

1. 启动用的伴生容器的方式 。

2. mysql config 之前配置的是具体的域名地址，比如 testdb.co, 但dns 解析有延迟，比如后端db出了问题，从库切主库，修改dns 解析ip， 常识就是新修改的dns 解析生效覆盖到所有网络节点需要时间，综上：dns 这种方式导致故障转义有延迟， db-proxy 直接配置成 127.0.0.1:3306即可。

3. 连接数固定，不会随着扩容导致连接数不可用。这是安利proxy给我最吸引的点，但

   ![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/image-20221104170702016.png)



好吧，伴生容器其实就是一个固定的连接池，所以它的功能是不是  = db config 配置的最佳实践



### Redis流出

### 背景

佩洛西访台的第二天突然告警

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/20221104171747.png)



overload 上的配置 : redis 带宽 100m, 超过100m 告警 。100m是我们单台redis 的带宽上限，超过带宽上限可能对我们这台redis 上key 的操作有影响。



### 过程

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/20221104171844.png)

1. moni上查看 其中单台redis 流量一直很高，只是18:49那会出现线性增长超过了100m， 开始告警。 怀疑这台redis 一直有大key。但overload 上 大key 和 慢日志都没有.
2. 拉长日志查询时间范围，筛选有问题的那台redis， 发现有个guard_all_uid 的慢查， 这个key内容是保存了所有用户uid 到bitmap 中，序列化成二进制，存入redis string。初步怀疑是这个key 导致
3. 数据库小姐姐拉出了这台机器上的大key，但强调不能确定当时的流量突增是这些大key 导致，发现 guard_all_uid 占用1.6m。
4. 冥冥中感觉是这个key 导致 开始梳理guard_all_uid 的逻辑，发现每台机器每秒会拉这个key， 所以判断redis 流量突增 是因为机器扩容，开始查询事件告警群消息，那个群有服务扩容缩容的记录， 如果真的是因为这个key 导致，xuser 一定在流量变化期间有机器数量的变动。 发现为了应对佩洛西访台后面的发酵，所以提前扩了一波xuser 的机器，时间点和 guard redis 流量突增的时间点吻合，当xuser 缩容的时候，guard redis 的流量开始减少。而且初步算下 40 * 1.6 m = 64m , 加上xuser-job 的写入 以及其他的缓存qps， 很容易突破100m



老的写入逻辑

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/20221104171929.png)



老的读取逻辑

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/20221104172005.png)



新的写入逻辑

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/20221104172028.png)



### 写入做的优化：

1.大key 按照尾数切分。 原始大key 1.6m， 切成10份后，每块是 0.2m。切分后数据覆盖写redis， 流量也会减少。（新增）

2. 晚上数据的释放。业务低谷期检查前一日过期数据， 确认 当前数据和 int32 数据都没有，则从bitmap 中移除。（不考虑hash 碰撞， 如果真的误删除，每2分钟脚本会重新构建新的缓存）。（新增）

3. 初始化bitmap 构建过程需要大概2分钟，这中间新购买的用户暂时只存在内存中，等构建完成，再把这批用户同时刷入内存中。（之前的代码只是在bitmap 不为空的时候操作，所以可能当bitmap 中只有2个uid 也会写入内存中，导致脏数据）（优化）

4. 定时删除新上的最近用户购买缓存。(新增)



可能存在的问题：

1.初始化数组的竞态写入。做了写锁， 还有那块的消费者，后续可以优化成一个。

2.bimap数组。数组元素的竞态写入。因为数组中元素已经加了锁，所以即使同时写同一个地址，应该也没有问题，现在用-race 检测会有异常，但单测并发并没有异常。



新的读取逻辑:

![](https://cytuchuang-1256930988.cos.ap-shanghai.myqcloud.com/20221104172118.png)

### 读取逻辑新的优化：

1.用户购买的时候添加新的缓存，zset， score 是时间戳， member 是用户uid。（新增）

2.启动的时候读取分片redis， 构建对应的bitmap。每秒更新全量bitmap， 改成每秒更新最近10s购买的用户uid, 通过redis 实现这部分内存共享。（新增）

3.每天从redis 更新一次全量的bitmap， 校准一次，防止一些脏数据产生。（新增）